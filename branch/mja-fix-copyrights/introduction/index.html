<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Why OpenMP offloading? &mdash; OpenMP for GPU offloading  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sphinx_lesson.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sphinx_rtd_theme_ext_color_contrast.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/overrides.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/togglebutton.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/minipres.js"></script>
        <script src="../_static/tabs.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Heat diffusion mini-app" href="../miniapp/" />
    <link rel="prev" title="OpenMP for GPU offloading" href="../" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../" class="icon icon-home"> OpenMP for GPU offloading
            <img src="../_static/ENCCS_CSC_logos.jpg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">The lessons</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Why OpenMP offloading?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#computing-in-parallel">Computing in parallel</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#distributed-vs-shared-memory-architecture">Distributed- vs. Shared-Memory Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="#processes-and-threads">Processes and threads</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#openmp">OpenMP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#execution-model">Execution model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-memory-model">OpenMP Memory Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-directives">OpenMP Directives</a></li>
<li class="toctree-l3"><a class="reference internal" href="#parallel-regions">Parallel regions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#work-sharing">Work sharing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#clauses">Clauses</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#data-sharing-attribute-clauses">Data sharing attribute clauses</a></li>
<li class="toctree-l4"><a class="reference internal" href="#synchronization-clauses">Synchronization clauses</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scheduling-clauses">Scheduling clauses</a></li>
<li class="toctree-l4"><a class="reference internal" href="#if-control">IF control</a></li>
<li class="toctree-l4"><a class="reference internal" href="#initialization">Initialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#reduction">Reduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#others">Others</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#runtime-library-routines">Runtime library routines</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-environment-variables">OpenMP environment variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiling-an-openmp-program">Compiling an OpenMP program</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../miniapp/">Heat diffusion mini-app</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-architecture/">Introduction to GPU architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profiling/">Profiling code for GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../target/">Offloading to GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data/">Data environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimization/">Optimizing OpenMP offloaded code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi-gpu/">Multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../interoperability/">Working alongside GPU libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../porting/">Porting code to OpenMP offloading</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick-reference/">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide/">Instructor’s guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../">OpenMP for GPU offloading</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home"></a> &raquo;</li>
      <li>Why OpenMP offloading?</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/openmp-gpu/blob/main/content/introduction.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="why-openmp-offloading">
<h1>Why OpenMP offloading?<a class="headerlink" href="#why-openmp-offloading" title="Permalink to this headline"></a></h1>
<div class="admonition-questions questions admonition">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>When and why should I use OpenMP offloading in my code?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Understand shared parallel model</p></li>
<li><p>Understand program execution model</p></li>
<li><p>Understand basic constructs</p></li>
</ul>
</div>
<div class="admonition-prerequisites prerequisites admonition">
<p class="admonition-title">Prerequisites</p>
<ol class="arabic simple">
<li><p>Basic C or FORTRAN</p></li>
</ol>
</div>
<div class="section" id="computing-in-parallel">
<h2>Computing in parallel<a class="headerlink" href="#computing-in-parallel" title="Permalink to this headline"></a></h2>
<p>The underlying idea of parallel computing is to split a computational problem into smaller subtasks. Many subtasks can then be solved <em>simultaneously</em> by multiple processing units.</p>
<div class="figure align-center" id="id1">
<img alt="../_images/compp.png" src="../_images/compp.png" />
<p class="caption"><span class="caption-text">Computing in parallel.</span><a class="headerlink" href="#id1" title="Permalink to this image"></a></p>
</div>
<p>How a problem is split into smaller subtasks depends fully on the problem. There are various paradigms and programming approaches how to do this.</p>
<div class="section" id="distributed-vs-shared-memory-architecture">
<h3>Distributed- vs. Shared-Memory Architecture<a class="headerlink" href="#distributed-vs-shared-memory-architecture" title="Permalink to this headline"></a></h3>
<p>Most of computing problems are not trivially parallelizable, which means that the subtasks need to have access from time to time to some of the results computed by other subtasks. The way subtasks exchange needed information depends on the available hardware.</p>
<div class="figure align-center" id="id2">
<img alt="../_images/distributed_vs_shared.png" src="../_images/distributed_vs_shared.png" />
<p class="caption"><span class="caption-text">Distributed- vs shared-memory parallel computing.</span><a class="headerlink" href="#id2" title="Permalink to this image"></a></p>
</div>
<p>In a distributed memory environment each computing unit operates independently from the others. It has its own memory and it  <strong>cannot</strong> access the memory in other nodes. The communication is done via network and each computing unit runs a separate copy of the operating system. In a shared memory machine all computing units have access to the memory and can read or modify the variables within.</p>
</div>
<div class="section" id="processes-and-threads">
<h3>Processes and threads<a class="headerlink" href="#processes-and-threads" title="Permalink to this headline"></a></h3>
<p>The type of environment (distributed- or shared-memory) determines the programming model. There are two types of parallelism possible, process based and thread based.</p>
<div class="figure align-center">
<img alt="../_images/processes-threads.png" src="../_images/processes-threads.png" />
</div>
<p>For distributed memory machines, a process-based parallel programming model is employed. The processes are independent execution units which have their <em>own memory</em> address spaces. They are created when the parallel program is started and they are only terminated at the end. The communication between them is done explicitly via message passing like MPI.</p>
<p>On the shared memory architectures it is possible to use a thread based parallelism.  The threads are light execution units and can be created and destroyed at a relatively small cost. The threads have their own state information but they <em>share</em> the <em>same memory</em> adress space. When needed the communication is done though the shared memory.</p>
<p>Both approaches have their advantages and disadvantages.  Distributed machines are relatively cheap to build and they  have an “infinite ” capacity. In principle one could add more and more computing units. In practice the more computing units are used the more time consuming is the communication. The shared memory systems can achive good performance and the programing model is quite simple. However they are limited by the memory capacity and by the access speed. In addition in the shared parallel model it is much easier to create race conditions.</p>
</div>
</div>
<div class="section" id="openmp">
<h2>OpenMP<a class="headerlink" href="#openmp" title="Permalink to this headline"></a></h2>
<p>OpenMP is de facto standard for threaded based parallelism. It is relatively easy to implement. The whole technology suite contains the library routines, the compiler directives and environment variables. The parallelization is done providing “hints” (directives) about the regions of code which are targeted for parallelization. The compiler then chooses how to implement these hints as best as possible. The compiler directives are comments in Fortran and pragmas in C/C++. If there is no OpenMP support in the system they become comments and the code works just as any other serial code.</p>
<div class="section" id="execution-model">
<h3>Execution model<a class="headerlink" href="#execution-model" title="Permalink to this headline"></a></h3>
<p>OpenMP API uses the fork-join model of parallel execution. The  program begins as a single thread of execution, the <strong>master</strong> thread. Everything is executed sequentially until the first parallel region
construct is encountered.</p>
<div class="figure align-center">
<img alt="../_images/threads.png" src="../_images/threads.png" />
</div>
<p>When a parallel region is encountered, master thread creates a group of threads, becomes the master of this group of threads, and is assigned the thread index 0 within the group. There is an implicit barrier at the end of the parallel regions.</p>
</div>
<div class="section" id="openmp-memory-model">
<h3>OpenMP Memory Model<a class="headerlink" href="#openmp-memory-model" title="Permalink to this headline"></a></h3>
<p>In the OpenMP API supports a relaxed-consistency shared-memory model. The <strong>global memory</strong> is a shared place where all threads can store and retrieve variables. In addition to it each thread has its own <strong>temporary view</strong> of the memory. The temporary view of memory can represent any kind of intervening structure, such as machine registers, cache, or other local storage, between the thread and the memory it  allows the thread to cache variables and  to avoid going to memory for every reference to a variable.  The temporary view of memory is not necesseraly consistent with that of other threads. Finally each thread has access to a part of memory that can not be access by the other threads, the <strong>threadprivate memory</strong>.</p>
<p>Inside a parallel region there are two kinds of access of the variables, <em>shared</em> and <em>private</em>. Each reference to a shared variable in the structured block becomes a reference to the original variable, while for each private variable referenced in the structured block, a new version of the original variable is created in memory for each thread. In the case of nested parallel regions a variable which private can be made shared to the inner parallel region.</p>
</div>
<div class="section" id="openmp-directives">
<h3>OpenMP Directives<a class="headerlink" href="#openmp-directives" title="Permalink to this headline"></a></h3>
<p>In OpenMP the compiler directives are specified by using <strong>#pragma</strong> in C/C++ or as special comments identified by unique sentinels in Fortran. Compilers can ingnore the OpenMP directives if the support for OpenMP is not enabled,</p>
<p>Here are some prototypes of OpenMP directives:</p>
<blockquote>
<div><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">C/C++</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Fortran</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#pragma omp directive [clauses]</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><div class="highlight-Fortran notranslate"><div class="highlight"><pre><span></span><span class="c">!$ omp directive [clauses]</span>
</pre></div>
</div>
</div></div>
</div></blockquote>
</div>
<div class="section" id="parallel-regions">
<h3>Parallel regions<a class="headerlink" href="#parallel-regions" title="Permalink to this headline"></a></h3>
<p>The compiler directives are used for various purposes: for thread creation, workload distribution (work sharing), data-environment management, serializing sections of code or for synchronization of work among the threads. The parallel regions are created using the <strong>parallel</strong> construct. When this construct is encounter additional thread are forked to carry out the work enclose in it.</p>
<div class="figure align-center" id="id3">
<img alt="../_images/omp-parallel.png" src="../_images/omp-parallel.png" />
<p class="caption"><span class="caption-text">Outside of a parallel region there is only one threas, while inside there are N threads</span><a class="headerlink" href="#id3" title="Permalink to this image"></a></p>
</div>
<p>All threads inside the construct execute the same, there is not work sharing yet.</p>
<blockquote>
<div><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-1-1-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-1-1-0" name="1-0" role="tab" tabindex="0">C/C++</button><button aria-controls="panel-1-1-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-1" name="1-1" role="tab" tabindex="-1">Fortran</button></div><div aria-labelledby="tab-1-1-0" class="sphinx-tabs-panel" id="panel-1-1-0" name="1-0" role="tabpanel" tabindex="0"><div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span><span class="cp"></span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="n">argv</span><span class="p">[]){</span><span class="w"></span>
<span class="cp">#pragma omp parallel</span>
<span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">   </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Hello world!&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="w"> </span><span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</div><div aria-labelledby="tab-1-1-1" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-1" name="1-1" role="tabpanel" tabindex="0"><div class="highlight-Fortran notranslate"><div class="highlight"><pre><span></span>  program hello
  integer :: omp_rank
!$omp parallel
  print *, &#39;Hello world!
!$omp end parallel
  end program hello
</pre></div>
</div>
</div></div>
</div></blockquote>
<p>Note that the value of the output from the <em>printf/print</em> can be all mixed up.</p>
</div>
<div class="section" id="work-sharing">
<h3>Work sharing<a class="headerlink" href="#work-sharing" title="Permalink to this headline"></a></h3>
<p>In a parallel region all threads execute the same code. The division of work can be done by the user, based on the thread id (or thread rank) different subtasks can be assigned to different threads, or by using the work-sharing constructs:</p>
<ul class="simple">
<li><p><em>omp for</em> or <em>omp do</em>: used to split up loop iterations among the threads, also called <em>loop</em> constructs.</p></li>
<li><p><em>sections</em>: assigning consecutive but independent code blocks to different threads</p></li>
<li><p><em>single</em>: specifying a code block that is executed by only one thread, a barrier is implied in the end</p></li>
<li><p><em>master</em> : similar to single, but the code block will be executed by the master thread only and no barrier implied in the end.</p></li>
<li><p><em>task</em>: allows to create units of work dynamically for parallelizing irregular algorithms such as recursive algorithms.</p></li>
<li><p><em>workshare</em>: divides the execution of the enclosed structured block into separate units of work. Each unit of work is executied by one thread.  (Fortran only)</p></li>
<li><p><em>simd</em>: indicates that multiple iterations of the loop can be executed concurrently using SIMD instructions</p></li>
</ul>
<p>Example of a trivially parallelizable problem using the <em>loop</em> workshare construct:</p>
<blockquote>
<div><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-2-2-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-2-2-0" name="2-0" role="tab" tabindex="0">C/C++</button><button aria-controls="panel-2-2-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-1" name="2-1" role="tab" tabindex="-1">Fortran</button></div><div aria-labelledby="tab-2-2-0" class="sphinx-tabs-panel" id="panel-2-2-0" name="2-0" role="tabpanel" tabindex="0"><div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span><span class="cp"></span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="n">argv</span><span class="p">[]){</span><span class="w"></span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="mi">1000</span><span class="p">];</span><span class="w"></span>
<span class="cp">#pragma omp parallel</span>
<span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="cp">#pragma omp for</span>
<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1000</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">i</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="w"> </span><span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</div><div aria-labelledby="tab-2-2-1" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-1" name="2-1" role="tabpanel" tabindex="0"><div class="highlight-Fortran notranslate"><div class="highlight"><pre><span></span>  <span class="k">program </span><span class="n">hello</span>
  <span class="kt">integer</span> <span class="kd">::</span> <span class="n">a</span><span class="p">[</span><span class="mi">1000</span><span class="p">]</span>
<span class="c">!$omp parallel</span>
<span class="c">!$omp do</span>
  <span class="k">do </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="mi">999</span>
    <span class="n">a</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span>
  <span class="k">enddo</span>
<span class="c">!$omp end do</span>
<span class="c">!$omp end parallel</span>
  <span class="k">end program </span><span class="n">hello</span>


<span class="n">In</span> <span class="n">this</span> <span class="n">example</span> <span class="n">OpenMP</span> <span class="n">distributes</span> <span class="n">the</span> <span class="n">work</span> <span class="n">among</span> <span class="n">the</span> <span class="n">threads</span> <span class="n">by</span> <span class="n">dividing</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">interations</span> <span class="n">in</span> <span class="n">the</span> <span class="n">loop</span> <span class="n">by</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">threads</span> <span class="p">(</span><span class="n">default</span> <span class="n">behaviour</span><span class="p">).</span> <span class="n">At</span> <span class="n">the</span> <span class="k">end </span><span class="n">of</span> <span class="n">the</span> <span class="n">loop</span> <span class="n">construct</span> <span class="n">there</span> <span class="k">is </span><span class="n">an</span> <span class="k">implicit </span><span class="n">synchronization</span><span class="p">.</span>
</pre></div>
</div>
</div></div>
</div></blockquote>
<p>The constructs can be combined if one is imediatly nested inside another construct.</p>
</div>
<div class="section" id="clauses">
<h3>Clauses<a class="headerlink" href="#clauses" title="Permalink to this headline"></a></h3>
<p>Together with compiler directives, OpenMP provides <strong>clauses</strong> that  can used to control the parallelism of regions of code. The clauses specify additional behaviour the user wants to occur and they refere to how the variables are visible to the threads (private or shared), synchronization, scheduling, control, etc. The clauses are appended in the code to the directives. Below is an list of many types of clauses available to the programmers:</p>
<div class="section" id="data-sharing-attribute-clauses">
<h4>Data sharing attribute clauses<a class="headerlink" href="#data-sharing-attribute-clauses" title="Permalink to this headline"></a></h4>
<p>By default all variables are <em>shared</em>. Sometimes <em>private</em> variables are necessary to avoid race conditions</p>
<blockquote>
<div><ul class="simple">
<li><p><em>shared</em>: the data declared outside a parallel region is shared, which means visible and accessible by all threads simultaneously. By default, all variables in the work sharing region are shared except the loop iteration counter.</p></li>
<li><p><em>private</em>: the data declared within a parallel region is private to each thread, which means each thread will have a local copy and use it as a temporary variable. A private variable is not initialized and the value is not maintained for use outside the parallel region. By default, the loop iteration counters in the OpenMP loop constructs are private.</p></li>
<li><p><em>default</em>: allows the programmer to state that the default data scoping within a parallel region will be either shared, or none for C/C++, or shared, firstprivate, private, or none for Fortran. The none option forces the programmer to declare each variable in the parallel region using the data sharing attribute clauses.</p></li>
<li><p><em>firstprivate</em>: like private except initialized to original value.</p></li>
<li><p><em>lastprivate</em>: like private except original value is updated after construct.</p></li>
<li><p><em>reduction</em>: a safe way of joining work from all threads after construct.</p></li>
</ul>
</div></blockquote>
<p>Bellow is an example of <em>reduction</em> code without race condition:</p>
<blockquote>
<div><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-3-3-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-3-3-0" name="3-0" role="tab" tabindex="0">C/C++</button><button aria-controls="panel-3-3-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-1" name="3-1" role="tab" tabindex="-1">Fortran</button></div><div aria-labelledby="tab-3-3-0" class="sphinx-tabs-panel" id="panel-3-3-0" name="3-0" role="tabpanel" tabindex="0"><div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#pragma omp parallel for shared(x,y,n) private(i) reduction(+:asum){</span>
<span class="w">   </span><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">       </span><span class="n">asum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">asum</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="w"> </span><span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-1" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-1" name="3-1" role="tabpanel" tabindex="0"><div class="highlight-Fortran notranslate"><div class="highlight"><pre><span></span><span class="c">!$omp parallel do shared(x,y,n) private(i) reduction(+:asum)</span>
   <span class="k">do </span><span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span>
      <span class="n">asum</span> <span class="o">=</span> <span class="n">asum</span> <span class="o">+</span> <span class="n">x</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">*</span><span class="n">y</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
   <span class="k">end do</span>
<span class="c">!$omp end parallel</span>
</pre></div>
</div>
</div></div>
</div></blockquote>
</div>
<div class="section" id="synchronization-clauses">
<h4>Synchronization clauses<a class="headerlink" href="#synchronization-clauses" title="Permalink to this headline"></a></h4>
<blockquote>
<div><ul class="simple">
<li><p><em>critical</em>: the enclosed code block will be executed by only one thread at a time, and not simultaneously executed by multiple threads. It is often used to protect shared data from race conditions.</p></li>
<li><p><em>atomic</em>: the memory update (write, or read-modify-write) in the next instruction will be performed atomically. It does not make the entire statement atomic; only the memory update is atomic. A compiler might use special hardware instructions for better performance than when using critical.</p></li>
<li><p><em>ordered</em>: the structured block is executed in the order in which iterations would be executed in a sequential loop</p></li>
<li><p><em>barrier</em>: each thread waits until all of the other threads of a team have reached this point. A work-sharing construct has an implicit barrier synchronization at the end.</p></li>
<li><p><em>nowait</em>: specifies that threads completing assigned work can proceed without waiting for all threads in the team to finish. In the absence of this clause, threads encounter a barrier synchronization at the end of the work sharing construct.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="scheduling-clauses">
<h4>Scheduling clauses<a class="headerlink" href="#scheduling-clauses" title="Permalink to this headline"></a></h4>
<blockquote>
<div><ul class="simple">
<li><p><em>schedule</em> (type, chunk): This is useful if the work sharing construct is a do-loop or for-loop. The iterations in the work sharing construct are assigned to threads according to the scheduling method defined by this clause. The three types of scheduling are:</p></li>
<li><p><em>static</em>: Here, all the threads are allocated iterations before they execute the loop iterations. The iterations are divided among threads equally by default. However, specifying an integer for the parameter chunk will allocate chunk number of contiguous iterations to a particular thread.</p></li>
<li><p><em>dynamic</em>: Here, some of the iterations are allocated to a smaller number of threads. Once a particular thread finishes its allocated iteration, it returns to get another one from the iterations that are left. The parameter chunk defines the number of contiguous iterations that are allocated to a thread at a time.</p></li>
<li><p><em>guided</em>: A large chunk of contiguous iterations are allocated to each thread dynamically (as above). The chunk size decreases exponentially with each successive allocation to a minimum size specified in the parameter chunk</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="if-control">
<h4>IF control<a class="headerlink" href="#if-control" title="Permalink to this headline"></a></h4>
<blockquote>
<div><ul class="simple">
<li><p><em>if</em>: This will cause the threads to parallelize the task only if a condition is met. Otherwise the code block executes serially.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="initialization">
<h4>Initialization<a class="headerlink" href="#initialization" title="Permalink to this headline"></a></h4>
<blockquote>
<div><ul class="simple">
<li><p><em>firstprivate</em>: the data is private to each thread, but initialized using the value of the variable using the same name from the master thread.</p></li>
<li><p><em>lastprivate</em>: the data is private to each thread. The value of this private data will be copied to a global variable using the same name outside the parallel region if current iteration is the last iteration in the parallelized loop. A variable can be both firstprivate and lastprivate.</p></li>
<li><p><em>threadprivate</em>: The data is a global data, but it is private in each parallel region during the runtime. The difference between threadprivate and private is the global scope associated with threadprivate and the preserved value across parallel regions.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="reduction">
<h4>Reduction<a class="headerlink" href="#reduction" title="Permalink to this headline"></a></h4>
<blockquote>
<div><ul class="simple">
<li><p><em>reduction</em> (operator | intrinsic : list): the variable has a local copy in each thread, but the values of the local copies will be summarized (reduced) into a global shared variable. This is very useful if a particular operation (specified in operator for this particular clause) on a variable runs iteratively, so that its value at a particular iteration depends on its value at a prior iteration. The steps that lead up to the operational increment are parallelized, but the threads updates the global variable in a thread safe manner. This would be required in parallelizing numerical integration of functions and differential equations, as a common example.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="others">
<h4>Others<a class="headerlink" href="#others" title="Permalink to this headline"></a></h4>
<blockquote>
<div><ul class="simple">
<li><p><em>flush</em>: The value of this variable is restored from the register to the memory for using this value outside of a parallel part</p></li>
<li><p><em>master</em>: Executed only by the master thread (the thread which forked off all the others during the execution of the OpenMP directive). No implicit barrier; other team members (threads) not required to reach.</p></li>
<li><p><em>collapse</em>: When more than one loop follows a <em>loop</em> construct it sppecifies how many loops in a nested loop should be collapsed into one large iteration space.</p></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="runtime-library-routines">
<h3>Runtime library routines<a class="headerlink" href="#runtime-library-routines" title="Permalink to this headline"></a></h3>
<p>The OpenMp includes and extensive suite of run-time routines. They can be used for many purposes: to modify/check the number of threads, detect if the execution context is in a parallel region, how many processors in current system, set/unset locks, timing functions, etc.</p>
<p>The functions definitions are in the <em>omp.h</em> header in C/C++ and in fortran in the <em>omp_lib</em> module.
Some very useful routines:</p>
<blockquote>
<div><ul>
<li><p><em>omp_get_num_threads()</em></p></li>
<li><p><em>omp_get_thread_num()</em></p></li>
<li><p><em>omp_get_wtime()</em></p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-4-4-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-4-4-0" name="4-0" role="tab" tabindex="0">C/C++</button><button aria-controls="panel-4-4-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-1" name="4-1" role="tab" tabindex="-1">Fortran</button></div><div aria-labelledby="tab-4-4-0" class="sphinx-tabs-panel" id="panel-4-4-0" name="4-0" role="tabpanel" tabindex="0"><div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span><span class="cp"></span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="n">argv</span><span class="p">[]){</span><span class="w"></span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">omp_rank</span><span class="p">;</span><span class="w"></span>
<span class="cp">#pragma omp parallel</span>
<span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">   </span><span class="n">omp_rank</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">omp_get_thread_num</span><span class="p">();</span><span class="w"></span>
<span class="w">   </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Hello world! by thread %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">omp_rank</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="w"> </span><span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</div><div aria-labelledby="tab-4-4-1" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-1" name="4-1" role="tabpanel" tabindex="0"><div class="highlight-Fortran notranslate"><div class="highlight"><pre><span></span>  <span class="k">program </span><span class="n">hello</span>
  <span class="k">use </span><span class="n">omp_lib</span>
  <span class="kt">integer</span> <span class="kd">::</span> <span class="n">omp_rank</span>
<span class="c">!$omp parallel</span>
  <span class="n">omp_rank</span> <span class="o">=</span> <span class="n">omp_get_thread_num</span><span class="p">()</span>
  <span class="k">print</span> <span class="o">*</span><span class="p">,</span> <span class="s1">&#39;Hello world! by thread &#39;</span><span class="p">,</span> <span class="n">omp_rank</span>
<span class="c">!$omp end parallel</span>
  <span class="k">end program </span><span class="n">hello</span>
</pre></div>
</div>
</div></div>
</li>
</ul>
</div></blockquote>
<p>The portability of the code can be mantained by using the conditional compilation  <strong>ifdef _OPENMP</strong>.</p>
</div>
<div class="section" id="openmp-environment-variables">
<h3>OpenMP environment variables<a class="headerlink" href="#openmp-environment-variables" title="Permalink to this headline"></a></h3>
<p>OpenMP standard defines also  a set of environment variables that all implementations have to support. The environment variables are set before the program execution and they are read during program start-up. They can be used to control the execution of the parallel code at run-time. They are used to set the number of threads, specify the binding of the threads or specify how the loop interations are divided.</p>
<p>Setting OpenMP environment variables is done the same way you set any other environment variables. For example:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>csh/tcsh</strong>:   setenv OMP_NUM_THREADS 8</p></li>
<li><p><strong>sh/bash</strong>:    export OMP_NUM_THREADS=8</p></li>
</ul>
</div></blockquote>
<p>Here are a few environment variables:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>OMP_NUM_THREADS</strong>:   Number of threads to use</p></li>
<li><p><strong>OMP_PROC_BIND</strong>:     Bind threads to CPUs</p></li>
<li><p><strong>OMP_PLACES</strong>:        Specify the bindings between threads and CPUs</p></li>
<li><p><strong>OMP_DISPLAY_ENV</strong>:   Print the current OpenMP environment info on stderr</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="compiling-an-openmp-program">
<h3>Compiling an OpenMP program<a class="headerlink" href="#compiling-an-openmp-program" title="Permalink to this headline"></a></h3>
<p>In order to use OpenMP the compiler needs to have support for it. The OpenMP support is enabled by adding an extra compiling option:</p>
<blockquote>
<div><ul class="simple">
<li><p>GNU: -fopenmp</p></li>
<li><p>Intel: -qopenmp</p></li>
<li><p>Cray: -h omp</p></li>
<li><p>PGI: -mp[=nonuma,align,allcores,bind]</p></li>
</ul>
</div></blockquote>
<div class="admonition-keypoints keypoints admonition">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>OpenMP is the de facto standard for programming shared memory machines</p></li>
<li><p>threaded based parallel programming model</p></li>
<li><p>fork-join model</p></li>
<li><p>global memory, temporary view, thread private memory</p></li>
<li><p>parallelism is exposed via directives which are treated as comments if no support</p></li>
<li><p>work sharing done via specific constructs</p></li>
<li><p>clauses provide additional control</p></li>
<li><p>runtime libray provides an extensive suite of routines</p></li>
<li><p>environment variables can be used to alter execution features of the the applications</p></li>
</ul>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../" class="btn btn-neutral float-left" title="OpenMP for GPU offloading" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../miniapp/" class="btn btn-neutral float-right" title="Heat diffusion mini-app" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, ENCCS and CSC.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>